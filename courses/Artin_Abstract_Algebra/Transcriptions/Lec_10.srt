1
00:00:12,480 --> 00:00:13,050
So,

2
00:00:14,300 --> 00:00:19,430
recall what we did last time, where we sort of left off.

3
00:00:20,460 --> 00:00:32,600
So, we showed that um

4
00:00:33,760 --> 00:00:38,860
if we start with a finite dimensional vector space,

5
00:00:41,760 --> 00:00:52,30
and a linearly independent subset of that vector space.

6
00:00:52,540 --> 01:02,820
So I'll call that set V, uh sorry S, and contain vectors V1 through VN, and assume that this is a linearly independent set.

7
01:04,130 --> 01:15,100
Um then S can be extended to a basis

8
01:16,480 --> 01:26,280
V1 VN, VN + 1 up to VM of V.

9
01:26,840 --> 01:42,810
Now this looks at first like some sort of simple, obvious result, and the proof wasn't very hard in some sense, but nonetheless, this is a very, very important result in so far as it completely informs the way we think about manipulating vector spaces.

10
01:43,240 --> 01:50,850
And it's going to have a major impact on our ability to analyze the sort of group theoretic properties of vector spaces.

11
01:51,380 --> 02:03,870
So, just to reinforce that idea, I'm going to look at some consequences uh of this particular result. Some of which we saw last time in some form, others which are somewhat new.

12
02:04,300 --> 02:16,220
So, let me just start by listing some consequences and we'll just run through briefly why those things are true.

13
02:16,760 --> 02:22,130
So, let's just start with the following.

14
02:22,880 --> 02:31,50
So let W be the subspace So I'm just going to retain notation from the recall here.

15
02:31,50 --> 02:50,370
And I'm going to let W be the subspace of V span by the set V1 through VN.

16
02:50,930 --> 03:01,140
So W is all things of the form C1V1 + dot dot dot CNVN as C1 through CN range over the implicit field here F.

17
03:01,540 --> 03:08,400
So, running behind the scenes here is that V is over F, F being some field.

18
03:08,950 --> 03:21,30
Um, and in general, it suffices to think of this field as being R, but that's not always true. Finite fields have weird properties, so you should bear that in mind, especially when you're adding finite sets of vectors, but

19
03:21,990 --> 03:25,650
for now we're just concerned with these sorts of general results.

20
03:26,250 --> 03:43,820
Okay, so and let W prime be the subspace span by the remaining vectors, VN + 1 through VM.

21
03:44,730 --> 03:47,210
Well, then here are two consequences.

22
03:47,840 --> 04:01,790
Then, first of all, the intersection of these things is zero, but that's not really a major result. The the point is more the thing that I'm about to write down, which sort of takes advantage of that fact.

23
04:03,40 --> 04:04,190
which is

24
04:05,320 --> 04:15,890
And we have a linear isomorphism between the product of W and W prime.

25
04:16,410 --> 04:26,900
What's the product? Well, this was part of the reading for one of the uh sections in the last chapter, but just to refresh your memories.

26
04:27,150 --> 04:38,160
This is the set of ordered pairs WW prime uh where W ranges over W, little W ranges over big W and little W prime ranges over big W prime.

27
04:39,370 --> 04:46,260
There's a linear isomorphism between this product thing and V.

28
04:47,250 --> 04:54,320
where we just take a pair WW prime and we map it to its sum.

29
04:56,310 --> 05:11,720
So, using a term introduced last time, can somebody give me a way of seeing fairly immediately that the intersection of W and W prime is just the zero vector space?

30
05:14,150 --> 05:14,580
Yes.

31
05:15,200 --> 05:17,780
So the W prime is W prime is spanning?

32
05:18,330 --> 05:27,10
Yes, so um that would be sort of yeah, it it has to do with spanning, but there's a much more critical thing.

33
05:27,340 --> 05:31,910
The thing that you really use immediately is linear independence.

34
05:32,290 --> 05:37,920
So it follows immediately from a sort of generalized notion of linear linear independence between these two things.

35
05:38,300 --> 05:44,120
Specifically the the fact that the cumulation of these two sets is linearly independent.

36
05:44,650 --> 06:14,620
Well, why? I mean, suppose I have something which is in W, while I can certainly write it by definition of span as A1 W1 up through AN WN, and if I have W prime, I can write that as AN + 1 WN + 1 up to AM WM. So these are just sort of general elements of W and W prime.

37
06:14,940 --> 06:28,280
And if these things are equal, so if if I have these things being equal, well, then I can subtract them. So W - W prime is zero.

38
06:28,730 --> 06:48,240
But that's this sort of non-trivial linear combination of A1W1 up through ANWN plus minus AN+1 WN+1 up through AMWM. So just negate that. This thing equals zero.

39
06:48,730 --> 07:17,640
And since W Sorry, these should be V's over here. V1 through VN, VN + 1 VM, V1 VN VN + 1 VM. Well, since these things are all linearly independent, by hypothesis, these things formed a basis, all of these AIs are zero. So each AI must be zero.

40
07:18,110 --> 07:24,250
So the things that we started with must have been zero.

41
07:25,190 --> 07:36,710
So it's the linear independence which is telling us in like a completely essential way that the intersection of these sorts of linearly independent spans must be zero.

42
07:37,360 --> 07:42,860
Now let's move on to the fact that this thing is a linear isomorphism.

43
07:43,490 --> 07:52,170
Well, the fact that it's just a homomorphism follows pretty much immediately from the definition of vector space structure on the product.

44
07:54,350 --> 07:58,140
So how do you add things in here? Well, you add them component wise.

45
07:58,580 --> 08:11,650
So if I have W1 W1 prime and W2 W2 prime, you just add these things, W1 and W2 W1 prime plus W2 prime.

46
08:12,190 --> 08:18,650
And if I multiply by a scalar stuff C * WW prime, that's just CW CW prime.

47
08:19,60 --> 08:25,960
So it's pretty apparent that this map here must at the very least be a linear transformation.

48
08:26,320 --> 08:27,30
So

49
08:28,140 --> 08:34,690
Linear transformation.

50
08:35,340 --> 08:37,90
Is that pretty clear to everyone?

51
08:37,90 --> 08:39,10
Okay.

52
08:39,730 --> 08:42,80
The surjectivity.

53
08:42,950 --> 08:46,700
In a word, where does the surjectivity come from?

54
08:49,730 --> 08:51,150
Spans.

55
08:51,580 --> 08:52,690
Spans, exactly.

56
08:53,160 --> 09:00,710
The fact that V1 up through VM spans.

57
09:01,250 --> 09:18,650
Because, you know, every everything in V is by definition some linear combination of V1 through VM. And so I take, you know, the first part of the linear combination from W and the second part of the linear combination from W prime, I add those up and I get whatever vector I wanted in V.

58
09:19,80 --> 09:22,350
So the surjectivity follows from spanning.

59
09:22,940 --> 09:28,10
And then finally, injectivity, where does that come from?

60
09:28,10 --> 09:28,810
Yes.

61
09:28,810 --> 09:29,470
Linear independence.

62
09:29,470 --> 09:31,150
Linear independence, exactly.

63
09:31,720 --> 1:03,90
In fact, we've already shown the critical thing about linear independence, which is specifically that the intersection of W and W prime is zero, because if I have um, you know, suppose I have uh W1 + W1 prime equaling W2 + W2 prime, right? Well, this implies that W1 - W2 is equal to W2 prime - W1 prime.

64
1:03,90 --> 1:04,590
This thing is in W.

65
1:04,840 --> 1:06,130
This thing is in W prime.

66
1:06,540 --> 1:12,670
So whatever this common vector is, it's in the intersection of W and W prime, which is zero.

67
1:13,100 --> 1:20,130
But if this thing is zero and this thing is zero, that implies that W1 equals W2 and W2 prime equals W1 prime.

68
1:20,530 --> 1:25,930
So the way we wrote these things as sums to start with was unique.

69
1:28,420 --> 1:32,600
Is that pretty clear? Are there any questions about this?

70
1:33,40 --> 1:33,470
Great.

71
1:41,840 --> 1:43,190
Second consequence.

72
1:45,660 --> 1:58,890
So now let's sort of forget about this sort of notation we introduced before. So we're just going to start with, you know, uh maybe the same vector space V, but we're not going to have W defined as before. We're just going to let W be some subspace.

73
1:59,270 --> 2:09,320
So, if W uh is a subspace of V,

74
2:10,190 --> 2:22,790
then there is another subspace W prime of V

75
2:26,110 --> 2:34,760
such that the composite map

76
2:35,480 --> 2:48,60
So I can certainly just inject W prime into V by just letting W prime map to W prime.

77
2:48,60 --> 2:54,210
I mean it's this is just sort of uh the the embedding map. This is sort of the identity map on W prime.

78
2:55,550 --> 3:04,180
And recall also that there is a canonical map, it's a surjection.

79
3:06,30 --> 3:14,100
There's a canonical quotient map, so canonical quotient map

80
3:14,540 --> 3:26,660
which just takes this W prime and maps it to the coset W prime the W coset of W prime

81
3:27,500 --> 3:33,200
such that this composite map is an isomorphism.

82
3:39,320 --> 3:40,200
So this is a great result.

83
3:40,800 --> 3:59,730
It's this is this is the result that was uh referred to at the very end of the last lecture and for which there was a sort of counter example given or part of which there was a counter example given to by uh a group theoretic uh general group theoretic example involving uh the cyclic groups that we saw last week.

84
3:59,960 --> 4:09,690
Um but it turns out that vector spaces are nice enough that things sort of always break up in this great way.

85
4:10,100 --> 4:18,140
And I'll I'll I'll elucidate that further with a third consequence, but can anybody just give me very briefly the argument for this?

86
4:20,170 --> 4:20,630
Yeah, Emily.

87
4:20,940 --> 4:29,260
Oh, it didn't take a basis for W, just extend it to basis for V and then the extra part of the basis will be the basis for W prime.

88
4:30,660 --> 4:32,110
Exactly. So paraphrasing.

89
4:32,890 --> 4:51,690
Um the point is that we take a basis call V1 through VN of W extend to basis VN + 1 up through VM of V.

90
4:52,290 --> 4:54,800
We can we're allowed to do this.

91
4:55,750 --> 5:13,230
Then we just let W prime be as before the span of Sorry. This should be V1 up through VM. Span of VN + 1 up through VM.

92
5:14,390 --> 5:22,170
And then it's not so hard to verify that this has exactly the right property.

93
5:22,690 --> 5:39,270
Um the thing is, well, certainly a homomorphism. I mean that it's a linear transformation follows just immediately by the definition. We chose these things to be linear transformations and the composite of a linear transformation is a linear transformation.

94
5:40,280 --> 5:54,900
The surjectivity follows from the fact that we saw before that everything in V can be written as a sum of something in W prime and something in W.

95
5:55,340 --> 6:08,140
So if we take some element uh uh of V mod W, then I can lift it to something in V, write it as something of the form W + W prime,

96
6:08,420 --> 6:17,720
where W is in W and W prime is in W prime. And then certainly the coset W prime plus big W must be the coset we started out with.

97
6:18,150 --> 6:19,220
Is that pretty clear?

98
6:21,160 --> 6:34,640
So surjectivity straightforward from definition of cosets and the spanning.

99
6:34,880 --> 6:45,150
So in general, in these sorts of results, whenever you see something that you need to be subjective, think about spanning. Whenever you need something to be injective, think about linear independence.

100
6:45,460 --> 6:55,200
These are just sort of general tips and tricks and this is certainly going to be the basic argument in all the consequences of the result from last time that we started with.

101
7:00,630 --> 7:07,200
And so injectivity, well, again, this is a sort of linear independence type result.

102
7:07,520 --> 7:18,750
So, if I have, I mean, I can certainly we just saw that I can certainly write any cosets um in the form W prime plus W where W prime is in big W prime.

103
7:19,250 --> 7:29,770
And so suppose I have two things which have the same image. Well, then we've seen that W1 prime - W2 prime must be in W.

104
7:30,470 --> 7:36,830
But it's also in W prime. And then again, by the same linear independence argument we saw before,

105
7:37,50 --> 7:42,760
this thing must be uh zero.

106
7:43,370 --> 7:46,770
So these must be the same uh cosets.

107
7:47,680 --> 7:48,670
Great.

108
7:49,220 --> 7:55,180
So let's look at a third consequence that brings together the first two consequences I've listed here.

109
7:56,480 --> 7:57,780
Yes, go ahead.

110
7:58,160 --> 7:59,960
Is that a capital W to the right of?

111
8:02,360 --> 8:05,710
So these are both Capital Ws.

112
8:17,920 --> 8:29,960
So the third thing is that if we put one and two together, put the arguments for one and two together,

113
8:32,500 --> 8:33,150
we get

114
8:35,240 --> 8:42,930
that for any subspace W of V, so suppose we have W subspace of V,

115
8:43,310 --> 8:52,250
there's an isomorphism between V and W with its product with the quotient.

116
8:52,750 --> 8:56,410
So V is isomorphic to W cross V mod W.

117
8:58,180 --> 9:10,930
And it's just an exercise. I mean, don't hand this in or anything, but think about when you're reading over your notes, why these two things come together to give this result.

118
9:12,330 --> 9:20,650
But it's just a very sort of cute result. It tells you exactly how to think about quotient and so on in uh in the case of vector spaces.

119
9:21,180 --> 9:27,380
Don't be fooled. I mean, in general, in groups, quotient structures can be very, very complicated, very, very difficult.

120
9:28,280 --> 9:35,710
But in the case of vector spaces, it's very, very easy to understand quotients as being sort of complements.

121
9:38,460 --> 9:40,720
And a fourth result,

122
9:41,830 --> 9:44,940
um, well, actually, before I move on to a fourth result,

123
9:45,850 --> 9:55,120
can somebody tell me something as a result of this about the dimensions of V W and V mod W? Yes.

124
9:56,30 --> 10:02,160
Yeah, the dimension of V has to be the sum of the dimension of W plus the dimension of V mod W.

125
10:02,590 --> 10:04,160
Yes, exactly.

126
10:04,540 --> 10:09,160
The dimension of V must be equal to the dimension of W plus the dimension of V mod W.

127
10:09,540 --> 10:15,710
Now, let's use this result on a general linear map.

128
10:24,220 --> 10:34,700
So on a general linear map, so if F V to U is a linear transformation,

129
10:36,480 --> 10:39,620
then,

130
10:40,150 --> 11:04,30
well, um two things are true. First of all, um V is isomorphic to the product of the kernel of F with the image of F.

131
11:04,350 --> 11:20,740
And consequently, again, by the same argument as here, the dimension of V is equal to the dimension of the kernel of F plus the dimension

132
11:21,230 --> 11:23,650
of the image of F.

133
11:24,890 --> 11:29,280
What group theoretic result implies this?

134
11:35,980 --> 11:36,370
Yes.

135
11:36,570 --> 11:50,170
Well, uh the result that um the order from the order of the kernel of uh meaning of H, the product by the order of its image is equal to R G.

136
11:50,480 --> 11:51,130
True.

137
11:51,550 --> 11:58,180
So, um that that would be true if you had like finite groups like say if this was over a finite field and we had a finite dimensional vector space.

138
11:58,460 --> 12:01,130
So what you were saying is you're referring to the counting formula that we looked at.

139
12:01,530 --> 12:04,220
But how did we prove that particular counting formula?

140
12:04,470 --> 12:06,0
There was a particular theorem that we saw.

141
12:06,690 --> 12:08,860
Yeah, there there was the isomorphism theorem.

142
12:09,220 --> 12:21,170
So the key result that we previously saw, and this is it can't be emphasized how important this result is because it in much the same way that our discussion here is completely informing the way we understand vector spaces.

143
12:21,560 --> 12:30,230
The isomorphism theorem we saw a few lectures ago completely informs the way we sort of decompose groups and understand their structure.

144
12:34,480 --> 12:39,940
So we saw a very significant isomorphism theorem.

145
12:40,530 --> 12:41,320
So why?

146
12:42,690 --> 13:01,970
Well, the isomorphism theorem for groups implies another sort of isomorphism theorem for vector spaces, where you just sort of add in the linear structure, which is something that's lacking in general groups. But here's a way of putting it. If I have a homomorphism, in fact, in our case, a linear transformation, because we want to preserve linear structure here, we're dealing with vector spaces. So given a linear transformation from some vector spaces V to U,

147
13:02,600 --> 13:10,860
the map F bar, remember we talked about the induced map, the induced map from V modulo

148
13:11,380 --> 13:24,230
the kernel of F going to the image of F

149
13:24,770 --> 13:41,400
where we take V plus the kernel of F, a typical element of V modulo some group is V plus that subgroup. So we take this and we map it to F of V, which is a well-defined element of the image subgroup or in our case subspace.

150
13:42,530 --> 13:43,970
Is this familiar to everyone?

151
13:44,410 --> 13:46,240
Does everybody remember the isomorphism theorem?

152
13:46,680 --> 13:47,830
It's a very important result.

153
13:48,570 --> 13:58,620
In fact, if you don't remember it, it's it's I mean, I can't emphasize enough. It's a really great idea to go home and try to figure out how to prove it without looking at the book. It's very important.

154
13:58,620 --> 14:06,50
So, the point is that this thing is a well-defined linear isomorphism.

155
14:06,50 --> 14:08,280
And we saw this in the case of groups.

156
14:08,280 --> 14:15,840
We saw that this thing is a well-defined homomorphism and uh that if this, yeah, the and that it's injective and surjective by sort of basic properties of cosets.

157
14:15,840 --> 14:21,0
And so the only thing that's sort of being added here is the linear part.

158
14:21,350 --> 14:26,0
And that just involves multiplication by scalars. You just have to see that this map preserves multiplication by scalars, and this isn't hard.

159
14:26,410 --> 14:32,970
So, given this result, all we have to do is take W to be the kernel of F.

160
14:33,420 --> 14:42,950
And then just put it in this result because this is saying that V is isomorphic to kernel of F times image of F. And this is saying that well, W times V mod W, but W is the kernel here, V mod W is V modulo the kernel, but that is we've just seen the image. So kernel times image.

161
14:52,830 --> 14:54,90
Any questions so far?

162
14:54,450 --> 14:57,970
So we're really just taking the composition of the two isomorphisms?

163
14:58,180 --> 15:01,150
Exactly. We're taking just a composition of the two isomorphisms.

164
15:01,620 --> 15:09,950
We're putting together all the things we know, we're accumulating them, we're putting together the isomorphisms, and we're seeing what we can find about the structure uh of the group.

165
15:10,360 --> 15:19,640
Now, in general, it's it's a very good idea if you're trying to understand the structure of a general group, and this is especially true of vector spaces, to try out various maps.

166
15:20,100 --> 15:21,930
Maps are giving you the structure.

167
15:22,280 --> 15:34,140
These sorts of theorems are telling you that you can take advantage of homomorphisms that you can find by various means to determine a lot about the structure of the vector spaces you started off with.

168
15:34,480 --> 15:39,190
They allow you to decompose maps and so on. Uh decompose the vector spaces and so on.

169
15:39,720 --> 15:50,950
Um this is very, very true in the case of groups. Uh you often can find some sort of homomorphism between groups and allow this to help you decompose the group into simpler structure.

170
15:52,860 --> 15:54,330
Okay, let's move on to something a little different.

171
16:02,260 --> 16:07,300
So, here's another sort of general problem for groups.

172
16:15,460 --> 16:24,630
Well, if I have a homomorphism between groups G and G prime say,

173
16:25,650 --> 16:31,940
we want to find the kernel and the image of this homomorphism.

174
16:32,460 --> 16:35,970
So a general problem, it's a sort of obvious thing that you'd want to be able to do.

175
16:36,400 --> 16:38,400
Now, this is in general very difficult.

176
16:38,810 --> 16:44,610
I mean, in fact, this is a sort of prerequisite for being able to take advantage of the kind of result that we're talking about here.

177
16:45,260 --> 16:57,210
But this can be very, very difficult in the case of general groups. This is generally difficult.

178
16:57,840 --> 17:00,960
But with vector spaces, it's easy.

179
17:01,150 --> 17:01,500
Yeah.

180
17:01,870 --> 17:02,350
Okay.

181
17:07,720 --> 17:14,690
And the main reason it's easy is because we have bases and hence we can write things in matrix form.

182
17:15,70 --> 17:25,140
And we have all these great algorithms like uh row reduction to help us find things like column spaces.

183
17:25,620 --> 17:40,120
So what I'm going to talk about for the rest of the lecture is how we make a translation between these sort of abstract vector spaces and the language of matrices.

184
17:40,490 --> 17:47,310
And then use all those matrix algorithms that you learned in basic linear algebra to then lift back to these abstract vector spaces that you so that you can solve problems like this.

185
17:47,750 --> 17:48,800
So here's the point.

186
17:49,240 --> 18:00,940
But we can solve in the case of vector spaces.

187
18:02,170 --> 18:16,720
The main tool The main tools are bases, matrices, and matrix algorithms.

188
18:18,10 --> 18:20,10
that you've all practiced ad nauseum.

189
18:20,340 --> 18:22,150
Yes, they are actually useful for something.

190
18:22,490 --> 18:23,200
So,

191
18:29,420 --> 18:32,170
let's start looking at what I mean by that.

192
18:33,160 --> 18:38,940
In order to do that, I want to set up some correspondences. I want to set up some dictionaries here.

193
18:39,960 --> 18:44,170
So here are some correspondences.

194
18:49,330 --> 18:58,130
The first correspondence I want to set up is one that we saw in some form in last lecture. Although it wasn't necessarily made completely explicit.

195
18:58,710 --> 19:04,920
So, at least it was only made explicit in one direction, but let's review it because it's so important.

196
19:05,270 --> 19:20,100
So we saw last lecture that if V is an n-dimensional vector space over a field F,

197
19:20,860 --> 19:34,380
then bases correspond to something. Bases correspond to linear isomorphisms between the sort of canonical n-dimensional vector space Fn and V.

198
19:37,410 --> 19:48,230
So, there is a correspondence, a one to one onto correspondence

199
19:48,650 --> 20:03,420
between bases of V and linear isomorphisms from Fn to V.

200
20:06,30 --> 20:11,690
So suppose I have a basis and we're going to assume that our bases are ordered.

201
20:14,390 --> 20:19,210
So maybe I'll just put parenthetically that these are ordered bases.

202
20:19,750 --> 20:25,190
So suppose I have my basis B, which consists of vectors V1 through VN.

203
20:26,170 --> 20:29,220
We defined last time an isomorphism. How did we define it?

204
20:30,840 --> 20:33,50
Does anybody remember?

205
20:34,440 --> 20:34,820
Yes.

206
20:35,160 --> 20:43,230
Uh we had the sum for any V equal to X1 times V1 etc. So we take uh the vector X1 comma X2.

207
20:43,630 --> 20:52,660
Exactly. Paraphrasing and introducing a bit of notation, which isn't standard, it's not in the book or anything, but it's going to be a useful notation for the purposes of this lecture.

208
20:53,220 --> 21:03,730
We're going to define something I'm going to call row B associated to this basis B. It's going to be going from Fn to V and what will it do? Well, it'll take

209
21:05,230 --> 21:12,390
the column vector. I'll write for the most part elements of Fn as column vectors.

210
21:12,890 --> 21:24,10
You take the column vector A1 through AN and you map it to the sum A1V1 plus ANVN.

211
21:24,750 --> 21:26,750
How do we go in the other direction?

212
21:27,160 --> 21:35,710
Suppose we have a linear isomorphism from Fn to V. What's the basis that corresponds to it?

213
21:36,180 --> 21:36,790
Yes, go ahead.

214
21:37,390 --> 21:47,10
It would be the inverse of the matrix whose columns it would be the inverse of the of F I guess what you're calling row B there.

215
21:47,970 --> 21:50,350
Yeah, that's one way of putting it.

216
21:50,810 --> 21:57,720
So another way of putting it so that we one doesn't make direct reference to B as being a matrix. Why don't we think of this as just being some sort of ordered set?

217
21:58,130 --> 22:03,700
Another way of putting exactly what you just said is that if we have some row,

218
22:04,290 --> 22:21,970
we take it to the ordered set row of 10000 and then row 0100 and then onwards to 001.

219
22:22,550 --> 22:31,160
So there's the standard basis of Fn and we just evaluate row on that standard basis.

220
22:31,560 --> 22:36,710
So this is just a way of sort of translating everything into standard basis language.

221
22:37,100 --> 22:46,660
The whole point of this, the whole point of why this works is precisely that Fn comes equipped with this sort of easy to work with basis.

222
22:47,200 --> 22:51,690
And a given abstract vector space doesn't necessarily.

223
22:52,150 --> 23:01,120
But by translating between this sort of canonical basis and whatever bases you might be able to find for your vector space, you're able to actually get work done.

224
23:01,510 --> 23:02,150
Okay.

225
23:02,630 --> 23:12,680
So, in a word, why is all of this true? Well, this just follows from the fact that the interpretation we gave to a basis last time

226
23:13,100 --> 23:21,650
is that every vector in the given vector space can be written uniquely as a linear combination of those basis elements.

227
23:22,120 --> 23:24,150
This is simply a reinterpretation of that.

228
23:24,600 --> 23:32,30
So anyway, I refer you to your notes from last time to review exactly what's going on here, but this follows from what we saw last time.

229
23:32,520 --> 23:41,190
Okay, the second correspondence I want to look at is something that you saw in basic linear algebra.

230
23:41,750 --> 23:54,350
And that is that linear transformations between our sort of standard vector spaces, Fn and Fm, these correspond to matrices.

231
23:54,780 --> 24:01,700
These correspond to M by n matrices over the field F.

232
24:08,830 --> 24:16,220
So another piece of notation just to make sure that we're all clear on what's going on here.

233
24:18,50 --> 24:26,160
I will denote the matrix associated to a linear transformation from Fn to Fm as square brackets F.

234
24:26,640 --> 24:28,740
What does square brackets F look like?

235
24:29,320 --> 24:39,190
Well, maybe in the language you saw before, this is the matrix associated to this with respect to the standard bases on these things.

236
24:39,640 --> 24:42,750
Well, that's just the matrix that looks like F

237
24:43,620 --> 24:54,170
of 10 dot dot dot And so this is an entire column here. So you have to think of this as being a matrix. So this isn't a one, this is just saying that this is an entire column.

238
24:55,410 --> 25:04,210
and then I just evaluate on all of the standard basis elements all the way to the end.

239
25:05,880 --> 25:10,960
And uh I mean this should be pretty familiar that this works. Going in the other direction.

240
25:14,760 --> 25:30,940
I take a matrix A and from it, I get a linear transformation from Fn to Fm by mapping a vector V in Fn to its matrix multiple by A.

241
25:31,480 --> 25:33,970
So this is just the standard thing we saw before.

242
25:34,520 --> 25:45,140
Now, one of the most important things about this correspondence is that it preserves the sorts of natural operations that you learned on both sides of that.

243
25:45,590 --> 25:56,660
So, on the side of the linear transformations here, you know that you can add linear transformations and you can scalar multiply and you can compose.

244
25:57,20 --> 26:01,970
And the point is that that's preserved by this taking of brackets.

245
26:02,420 --> 26:12,150
So, in brief, C1F1 + C2F2 equals C1 brackets F1 + C2 brackets F2.

246
26:12,590 --> 26:16,670
If I take the matrices associated to these things, it preserves all this sort of structure.

247
26:17,160 --> 26:23,350
And moreover, if I compose F1 and F2, well that just corresponds to matrix multiplication.

248
26:23,850 --> 26:30,970
This is very important because we know exactly how to do matrix multiplication, we know how to do all these things in very simple, efficient algorithmic way.

249
26:31,550 --> 26:36,10
So this is a way of just translating into the language of matrices.

250
26:36,420 --> 26:40,920
So let's sort of put these things together, put these two correspondences together.

251
26:47,430 --> 26:56,420
So if I take correspondence is one and two and I put them together,

252
26:56,980 --> 27:06,190
what do I get? Well, given some linear transformation from say V to V prime.

253
27:06,480 --> 27:09,730
So this is just some linear transformation.

254
27:10,200 --> 27:22,740
And let's assume that this thing has dimension N and M. And let's just equip these things with bases B and B prime.

255
27:23,260 --> 27:31,150
So these are these finite dimensional vector spaces of dimension N and M and I equip them with these bases B and B prime. We know we can always equip some a vector space with a basis.

256
27:32,410 --> 27:49,170
Um at least we show that in the finite dimensional case. There is a proof that's more general that works in sort of the most general uh uh version of the theory of vector spaces where you allow infinite dimensional vector spaces, but that requires a little set theory.

257
27:49,640 --> 27:50,410
So,

258
27:51,290 --> 27:54,10
And the controversial axiom of choice. So,

259
27:55,910 --> 27:58,390
Um given this thing, what do we do?

260
27:58,690 --> 28:05,220
Well, we have the matrix of F

261
28:05,770 --> 28:10,990
with respect to B and B prime.

262
28:11,520 --> 28:17,20
How do we define this? Well, let's just give it a notation, FB B prime.

263
28:17,450 --> 28:26,420
This thing is rho B prime inverse F rho B.

264
28:26,770 --> 28:28,10
Why does this make sense?

265
28:28,460 --> 28:39,380
Well, we defined rho B so that it takes this, you know, standard vector space, these things of the form Fn to the vector space we're working in.

266
28:39,760 --> 28:58,980
Then F takes the thing that uh the vector space we're working in V to something in the vector space V prime. Then rho B prime translates back out of the abstract language of these vector spaces V prime and gives you something in this sort of canonical uh vector space Fm

267
28:59,360 --> 29:01,380
using this basis B prime.

268
29:01,840 --> 29:10,160
So it's the matrix of F with respect to B and B prime. Another sort of explicit way of looking at this is um explicitly,

269
29:10,520 --> 29:20,950
if B is V1 through VN and B prime is say V1 or maybe make this easier, W1

270
29:21,340 --> 29:31,10
through WM, then, well,

271
29:31,990 --> 29:49,690
given any of these VJs, we know that we can write its image uniquely as a linear combination of things in this basis B prime. That was the defining characteristic of bases. So this thing looks like just the sum uh um for each of these Js, J is just ranging over one through n.

272
29:49,990 --> 30:00,720
This looks like the sum I equals 1 to M AIJ uh WI for some scalars AIJ.

273
30:01,270 --> 30:10,390
So what is this thing F B B prime? Well,

274
30:12,400 --> 30:23,160
it's just this matrix made up of these numbers. And it's a very easy verification from this definition.

275
30:23,520 --> 30:39,400
This definition is just a sort of fancy and easy to work with way of introducing this more computational way of referring to the matrix with respect to B and B prime.

276
30:42,110 --> 30:43,100
Okay.

277
30:44,710 --> 30:48,990
What if we want to change bases? I mean, there's nothing canonical about a basis.

278
30:49,500 --> 30:52,680
There are lots and lots of bases for a given vector space.

279
30:53,50 --> 31:03,750
Sometimes you need to work in different vector uh in different bases uh according to the application. So sometimes there are more convenient uh uh bases than the one that you started working with.

280
31:04,10 --> 31:09,270
So suppose in the middle of your calculations, you need to start changing bases. How do you do this?

281
31:09,710 --> 31:16,240
Well, again, this is just a souped up version of exactly what you saw in basic linear algebra.

282
31:16,670 --> 31:24,270
This is just putting all the trappings of this theory of abstract vector spaces over an arbitrary field on a theory that you've already seen.

283
31:25,180 --> 31:30,930
So, change of basis.

284
31:32,30 --> 31:44,370
So suppose we have V um and it has bases B1 and B2.

285
31:44,850 --> 31:53,220
And we have vector space vector space V prime over F and it has bases B1 prime and B2 prime.

286
31:56,300 --> 32:09,980
Well, let's just sort of write out explicitly what F is in terms of the second vector space. So suppose you write this in terms of B2, B2 prime.

287
32:10,480 --> 32:21,220
Well, we defined this as being rho B2 prime inverse F rho B2.

288
32:21,680 --> 32:23,140
Right?

289
32:24,560 --> 32:35,200
But I can always just sort of insert here say uh rho B1 prime, rho B1 prime inverse.

290
32:35,530 --> 32:37,10
That's just the identity map. So I'm certainly allowed to insert it here.

291
32:37,440 --> 32:42,140
And I can do the same sort of thing over here. So let me just write that out completely explicitly.

292
32:42,650 --> 32:50,670
I can rewrite this as rho B2 prime uh inverse rho B1 prime times rho B1

293
32:51,70 --> 32:57,100
prime inverse F rho B1

294
32:58,210 --> 33:14,200
uh rho B1 inverse rho B2. Why did I do this?

295
33:15,880 --> 33:22,710
Well, stuck in the middle here is F with respect to the bases B1 prime and B1.

296
33:23,150 --> 33:26,240
This thing is exactly equal to

297
33:28,510 --> 33:32,150
rho B1 uh sorry, B2 inverse rho B1 times F

298
33:32,490 --> 33:42,210
with respect to B1 prime and B1 times rho B1 inverse rho B2.

299
33:42,760 --> 33:48,980
And notice that the things on either side of this look very, very similar.

300
33:49,310 --> 33:51,200
We need primes on the left?

301
33:51,550 --> 33:52,800
Um, yeah, I mean the primes should certainly be

302
33:53,160 --> 33:54,300
on the left. Thank you.

303
33:55,820 --> 33:56,970
Absolutely.

304
33:57,630 --> 34:02,190
So things that look like this, it makes sense to call change of basis matrices.

305
34:02,550 --> 34:21,380
So, let's just give it that moniker. So, call something of the form rho B1 inverse rho B2

306
34:21,790 --> 34:27,970
the change of basis matrix.

307
34:28,310 --> 34:29,470
or A change of basis matrix.

308
34:29,750 --> 34:32,180
In fact, it's unique given the B2 and the B1, but

309
34:36,40 --> 34:40,280
Now, this is a bit general.

310
34:40,910 --> 34:51,330
You've probably seen this construction in a essentially identical form, but in the special case where V and V prime are the same.

311
34:52,490 --> 34:53,370
So,

312
34:55,290 --> 34:58,690
um if V equals V prime,

313
34:59,250 --> 35:11,10
then well, I could just rewrite this as B2, B2. So I mean again, I just have B1, B2. And so now these are bases for both V and V prime.

314
35:11,450 --> 35:19,280
This thing just looks like uh rho B1 inverse rho B2

315
35:19,710 --> 35:28,230
all inverse because that looks like rho B2 inverse rho B1. And I could remove the primes here because I'm just taking the same vector space.

316
35:28,580 --> 35:37,460
times F with respect to B1 and B1 times rho B1 inverse rho B2.

317
35:38,20 --> 35:41,250
So this is just the the the idea of similar matrices.

318
35:41,640 --> 35:50,820
This is something of the form P F B1 B1 P inverse rather, F B1 B1 P

319
35:51,300 --> 35:53,940
where P is exactly this change of basic basis matrix.

320
35:55,670 --> 36:00,970
Now, stepping back to the general construction for just a second,

321
36:01,670 --> 36:14,680
let's just explicitly write down for sort of calculation purposes what that thing is. Well,

322
36:15,270 --> 36:22,210
if I have B1 and I can write it as V1 through VN, and I have B2

323
36:22,640 --> 36:28,50
and I write it as W1 through WN, then rho B1 inverse

324
36:28,410 --> 36:33,310
rho B2 is just the matrix CIJ

325
36:33,920 --> 36:49,100
where again, use this sort of defining property of bases to write each element WJ as a linear combination of the VIs, which I can certainly do.

326
36:49,440 --> 36:53,660
So this is uh I ranging over 1 to n.

327
36:56,690 --> 36:57,980
That's calculate

328
36:58,950 --> 37:02,140
this is what you do.

329
37:03,960 --> 37:15,130
So, if I just sort of think about this kind of thing that we've written down here, we've actually given this sort of thing a name.

330
37:15,440 --> 37:19,980
I mean, in basic linear algebra, you call this like the theory of similar matrices.

331
37:20,310 --> 37:24,200
But so far in group theory, we've given this a different name. That's conjugation of matrices.

332
37:24,590 --> 37:33,660
So what you have to start doing is in the, you know, in in this sort of course, you want to start moving away from that language of like similar matrices and to the language of group theory.

333
37:33,970 --> 37:36,730
So you would talk about this as being conjugacy.

334
37:37,120 --> 37:42,700
And in fact, this totally makes sense because we actually have a group in which we're doing this conjugation.

335
37:43,110 --> 37:44,680
Let me make that explicit.

336
37:50,560 --> 37:56,200
So again, let's just start with some vector space V.

337
37:56,950 --> 38:01,970
So V is just some vector space.

338
38:02,770 --> 38:14,230
Well, then we define GLV to be the set of linear isomorphisms from V to V.

339
38:15,50 --> 38:19,940
So this is sort of the analog of the automorphism group of a group.

340
38:20,460 --> 38:29,350
Except that we demand something additional. We demand not only this thing be a group isomorphism, but that it preserve the scalar multiplication.

341
38:29,650 --> 38:31,720
So these are linear isomorphisms.

342
38:32,260 --> 38:41,940
But in particular, you can think of this as being some subgroup of the automorphism group of V just viewed as a group.

343
38:43,660 --> 38:53,710
So put another way, these are just the invertible linear transformations from V to V.

344
38:56,20 --> 39:17,170
Or using a language that we've seen several times, what did we call the invertible elements of Z mod NZ? Exactly, Z mod NZ star. So it actually makes sense given the notation we've introduced so far to call this H V to V star.

345
39:18,580 --> 39:29,190
Now remember, the whole point of what we were just showing is that H V to V, we can sort of translate into a new language, the language of matrices,

346
39:29,500 --> 39:31,710
given a choice of basis.

347
39:32,170 --> 39:32,870
So,

348
39:37,840 --> 39:42,970
given a basis B of V,

349
39:43,880 --> 39:53,250
we just saw we can rewrite this isomorphically as the set of invertible

350
39:54,830 --> 4:08,150
matrices in uh uh in MN by N F where n is the dimension of V.

351
4:08,570 --> 4:11,920
What is this thing called? GLNF.

352
4:13,750 --> 4:23,760
So the idea is that we have this abstract thing, GLV, but when we need to work with it,

353
4:24,150 --> 4:28,920
it's much more convenient to work with matrices because you can actually do calculation with matrices.

354
4:29,300 --> 4:34,120
And given a choice of basis, you're, you know, you can easily do this.

355
4:34,430 --> 4:39,200
You have an induced isomorphism rather with GLNF.

356
4:39,530 --> 4:44,250
And so it now becomes very easy to to do whatever you want.

357
4:45,610 --> 4:46,290
Okay.

358
4:47,570 --> 4:50,260
Um the general point here is that you can set up this sort of dictionary.

359
4:50,630 --> 4:54,210
Just one last question to to sort of feed your thought.

360
4:54,640 --> 4:59,270
In this language, in this sort of translation, what is the image

361
4:59,640 --> 5:01,480
of a homo of a linear transformation

362
5:02,150 --> 5:03,540
in terms of matrices?

363
5:04,640 --> 5:05,210
Yes.

364
5:05,520 --> 5:06,980
The set of column spaces?

365
5:07,310 --> 5:08,610
The column space, exactly.

366
5:09,100 --> 5:14,170
The image under this sort of dictionary is the same thing as the column space.

367
5:14,760 --> 5:18,310
We know how to find the column space of a matrix.

368
5:18,700 --> 5:24,980
So given an abstract vector space, we now have an algorithm for finding the image of a given linear transformation.

369
5:25,610 --> 5:27,770
This same thing applies to the kernel.

370
5:28,160 --> 5:37,410
Because you can find the uh um the null space of a matrix and that thing under this dictionary is precisely the kernel.

371
5:37,730 --> 5:38,950
So that's just food for thought.